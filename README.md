## ğŸ¦™Offline Conversational AI with Llama3.2 via Ollama - RAG - FAISS

Offline, privacy-first AI assistant using a **local LLaMA 3.2** via **Ollama model**, FAISS memory, and document-grounded retrieval. Ask any questions and as well as questions about your PDFs and get precise, context-aware answers â€” all offline.

## ğŸš€ Features

- Local LLM inference with LLaMA 3.2 (via Ollama)
- Retrieval-Augmented Generation (RAG) using FAISS
- PDF ingestion: upload multiple PDFs as knowledge sources
- Conversational memory: remember previous messages
- Offline & privacy-safe: no cloud APIs needed
- Interactive UI: built with Streamlit
- Extensible & modular: easy to add voice, additional embeddings, or new models

  
### ğŸ—‚ Project Structure

```
LocalRAG-LLM/
â”‚
â”œâ”€ app.py                # Main Streamlit app
â”œâ”€ ollama_client.py      # Handles communication with local LLaMA
â”œâ”€ prompt.py             # SYSTEM_PROMPT for LLaMA instructions
â”œâ”€ requirements.txt      # Project dependencies
â”‚
â”œâ”€ rag/
â”‚   â”œâ”€ __init__.py
â”‚   â”œâ”€ pdf_loader.py     # Extracts text from PDFs
â”‚   â””â”€ rag_store.py      # Adds/retrieves document chunks in memory
â”‚
â”œâ”€ memory/
â”‚   â”œâ”€ __init__.py
â”‚   â””â”€ faiss_memory.py   # FAISS vector store + embeddings + retrieval
â”‚
â””â”€ data/
    â””â”€ doc.pdf           # Example PDF storage folder
```

## ğŸ›  Installation

1. Clone the repository:

```
git clone https://github.com/sowmya13531/LocalRAG-LLM.git
cd LocalRAG-LLM
```

2. Create a virtual environment (optional but recommended)
```
python -m venv venv
source venv/bin/activate      # Linux/Mac
venv\Scripts\activate         # Windows
```

4. Install dependencies
```
pip install -r requirements.txt
```

4. Make sure Ollama is installed and LLaMA 3.2 model is ready locally.

## âš™ï¸ Usage

1. Run the Streamlit app:

*streamlit run app.py*

2. Upload PDF documents under â€œğŸ“„ Upload Docâ€.

3. Ask questions in the chat area under â€œğŸ’¬ Chat with Local RAG-LLMâ€.

4. LLaMA generates document-grounded answers, and the conversation is stored in memory.

## ğŸ’¡ How It Works (Pipeline)

1. PDF Ingestion: pdf_loader.py extracts text from uploaded PDFs.

2. Chunking & Memory Storage: rag_store.py splits text into chunks and stores in FAISS memory using faiss_memory.py.

3. Query Handling:
User submits a question.
Relevant document chunks are retrieved with semantic similarity (retrieve_docs).

**SYSTEM_PROMPT + retrieved context + user query is sent to LLaMA (ollama_client.py).**

4. LLM Response: LLaMA generates an answer that is document-grounded, clear, and concise.

5. Memory Update: Both user input and assistant reply are stored in FAISS memory for multi-turn conversation.

## ğŸ–¤ SYSTEM_PROMPT

Controls assistant behavior:

*You are a helpful AI assistant.
Use conversation memory and provided documents when relevant.
If documents are provided, base answers strictly on them.
Be clear and concise.*

- Ensures factual, context-aware responses

- Avoids hallucinations

- Maintains consistent style


### âš¡ Tech Stack

- Python 3.10+
- Streamlit â€” interactive UI
- FAISS â€” fast semantic search
- Ollama + LLaMA 3.2 â€” local LLM inference
- PyPDF2 / pypdf â€” PDF parsing
- NumPy + sentence-transformers â€” embeddings
- Optional: Gradio, pyttsx3, speechrecognition, pydub for voice features

### ğŸ“ˆ Improvements / Future Work
- Replace random embeddings with real embeddings (sentence-transformers or Ollama embeddings)
- Support voice input/output
- Advanced chunking with sliding window for better retrieval
- Multi-document conflict handling & metadata filtering
- Save & load FAISS index to persist knowledge across sessions
- Streaming responses for better UX
